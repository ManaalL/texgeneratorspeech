{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "outputs": [],
      "source": [
        "path_to_file = '/content/speech.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bf290f-b2c1-4d85-b16b-3208ab2ca8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 41570 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3942424-2b7b-4312-9c44-406477c45e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEVE JOBS,  JUNE 14 2005, STANFORD  I am honored to be with you today at your commencement from one of the finest universities in the world. I never graduated from college. Truth be told, this is the closest I've ever gotten to a college graduation.\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eeb8f07-7614-4468-93a6-b31fc1afbf90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926fd5da-1ce2-47c3-8ccb-be639966ccc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f53a7d3-ec1b-44c4-857d-a22b109dfca1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[48, 49, 50, 51, 52, 53, 54], [71, 72, 73]]>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7b90a3-24ec-4877-8054-eeb57bec2fc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1862de-12fc-47c6-ddf9-60c6d209a1ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc782de-045f-4022-a0d9-b5076620cca3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(41570,), dtype=int64, numpy=array([41, 42, 28, ..., 50, 58, 10])>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjH5v45-yqqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a4eca7-c9fb-436b-e561-a5abeae47670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S\n",
            "T\n",
            "E\n",
            "V\n",
            "E\n",
            " \n",
            "J\n",
            "O\n",
            "B\n",
            "S\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af9972d-1259-44fc-c7c7-b5a334fd27f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'S' b'T' b'E' b'V' b'E' b' ' b'J' b'O' b'B' b'S' b',' b' ' b' ' b'J'\n",
            " b'U' b'N' b'E' b' ' b'1' b'4' b' ' b'2' b'0' b'0' b'5' b',' b' ' b'S'\n",
            " b'T' b'A' b'N' b'F' b'O' b'R' b'D' b' ' b' ' b'I' b' ' b'a' b'm' b' '\n",
            " b'h' b'o' b'n' b'o' b'r' b'e' b'd' b' ' b't' b'o' b' ' b'b' b'e' b' '\n",
            " b'w' b'i' b't' b'h' b' ' b'y' b'o' b'u' b' ' b't' b'o' b'd' b'a' b'y'\n",
            " b' ' b'a' b't' b' ' b'y' b'o' b'u' b'r' b' ' b'c' b'o' b'm' b'm' b'e'\n",
            " b'n' b'c' b'e' b'm' b'e' b'n' b't' b' ' b'f' b'r' b'o' b'm' b' ' b'o'\n",
            " b'n' b'e' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO32cMWu4a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6df9101-145d-4c8b-a7c3-5c9885f8c659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'STEVE JOBS,  JUNE 14 2005, STANFORD  I am honored to be with you today at your commencement from one '\n",
            "b'of the finest universities in the world. I never graduated from college. Truth be told, this is the c'\n",
            "b\"losest I've ever gotten to a college graduation. Today I want to tell you three stories from my life.\"\n",
            "b\" That's it. No big deal. Just three stories.   The first story is about connecting the dots.  I dropp\"\n",
            "b'ed out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 m'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a106b58f-0ac0-43d3-ae5b-c74b0de0f4a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c505876b-9b05-43c2-ed08-f6d958051f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'STEVE JOBS,  JUNE 14 2005, STANFORD  I am honored to be with you today at your commencement from one'\n",
            "Target: b'TEVE JOBS,  JUNE 14 2005, STANFORD  I am honored to be with you today at your commencement from one '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d3ce78-7db7-433d-f224-992876e00c00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKbfm04amhXk"
      },
      "source": [
        "Note: For training you could use a `keras.Sequential` model here. To  generate text later you'll need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a720345-b2ac-4add-e99f-47d6f839bb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 82) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a25346-367e-4caf-c6be-4df0a77a078b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  20992     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  84050     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,043,346\n",
            "Trainable params: 4,043,346\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3931b975-210c-4878-d763-dc08bd5c51b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([47, 69,  7,  5, 45, 23, 70, 12, 26, 32, 61, 31, 48, 53,  4, 41, 16,\n",
              "       39,  5, 72, 58, 62, 50,  8, 54, 80, 33, 33, 41,  1, 20, 62, 78, 30,\n",
              "       38, 17, 52, 26,  2, 56, 63, 10, 18, 30, 74,  1,  9, 34,  1, 56, 74,\n",
              "       21,  8, 35,  7, 10, 11, 68, 44, 18,  5, 35, 38, 61, 72, 53, 43, 19,\n",
              "        5, 39, 46,  3, 46, 51, 14, 39, 58, 27, 12, 44, 80, 25, 57, 71, 49,\n",
              "       20, 70, 41, 43, 32, 11, 31, 30, 48, 13, 47, 57, 17, 49, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b800d39-b826-47f0-b6c5-44b622edd477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'y in Albuquerque that had begun making the world\\xe2\\x80\\x99s first personal computers. I offered to sell them '\n",
            "\n",
            "Next Char Predictions:\n",
            " b'Yv\\'$W?w1CInHaf\"S5P$ykoc,g\\xe2\\x80\\x9dJJS\\n9o\\xe2\\x80\\x99GO6eC ip.7G\\xc2\\xa2\\n-K\\ni\\xc2\\xa2:,L\\'.0uV7$LOnyfU8$PX!Xd3PkD1V\\xe2\\x80\\x9dBjxb9wSUI0HGa2Yj6b4'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f75311-2e37-4825-b173-5d0295774ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 82)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.407855, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c134d71-7b22-4582-9065-d7c18257e8a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82.09319"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9352f4ba-c934-4a7f-ac42-83ad1c16c354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "6/6 [==============================] - 2s 87ms/step - loss: 4.5086\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 4.0855\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 72ms/step - loss: 3.7658\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 72ms/step - loss: 3.2637\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 3.0434\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 73ms/step - loss: 2.9350\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 2.8585\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 2.7570\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 2.6604\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 72ms/step - loss: 2.5712\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 73ms/step - loss: 2.5074\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 1s 78ms/step - loss: 2.4546\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 2.4102\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 2.3737\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 2.3472\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 2.3217\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 2.2974\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 2.2779\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 73ms/step - loss: 2.2560\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 2.2369\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 2.2233\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 2.2018\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 2.1806\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 73ms/step - loss: 2.1615\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 2.1438\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 73ms/step - loss: 2.1213\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 2.0994\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 2.0777\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 2.0510\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 2.0260\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 2.0023\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 1.9744\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 1.9538\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 1.9288\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 1.9043\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 1.8738\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 1.8459\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 1.8235\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 1.7908\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 1.7674\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 1.7362\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 1.7147\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 1.6861\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 1.6539\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 1.6333\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 1.6025\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 1.5731\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 1.5408\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 1.5064\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 1.4795\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 1.4476\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 1.4149\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 1.3806\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 1.3525\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 1.3189\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 73ms/step - loss: 1.2830\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 1.2507\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 1.2077\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 1.1715\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 1.1340\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 1.0986\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 1.0579\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 1.0181\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.9775\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 1s 80ms/step - loss: 0.9329\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.8885\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 0.8454\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.8054\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 1s 79ms/step - loss: 0.7603\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.7150\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.6755\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 0.6311\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 0.5859\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.5442\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 1s 79ms/step - loss: 0.5060\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 0.4735\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 0.4301\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.3942\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3594\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 0.3285\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.3002\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.2738\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2481\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.2257\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.2078\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.1901\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.1768\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 1s 78ms/step - loss: 0.1631\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.1512\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.1406\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.1310\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.1223\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.1154\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 75ms/step - loss: 0.1082\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 0.1028\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 0.0982\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 0.0932\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 76ms/step - loss: 0.0890\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 0.0859\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.0831\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13ebde5-c5e9-4aa5-b3ca-0b6f5ee4af07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: be abue to think the laves of the United States.\n",
            "\n",
            "We were shock.\n",
            "\n",
            "We deneral grvan trans to asswer had never grade that she say of manicabed to be able to show that a program is vaccinating millions more children. You have to be able to show that a program is vaccinating millions efreeting that well, the sindler essences on Marrially, I really dadn’t rus and time you run it ont that my mother had never graduated from college. Truth be toive to gonate to in prow-con lives of deepasity of elerable dissanity. It was awful tast, and intuity be a valievingo.\n",
            "\n",
            "But for you. It was tough going there in the beginning. Because initially – the initial thought with PayPalwanth. \n",
            "\n",
            "\n",
            "““All ination of esperience especially striking was that I had have would have them. If I had sped that with the lasted taking live from dightented on my you will judge yourselves not on your professional accomplishments alone, but also on how well you theared conglimation ofteries the millions of childrengly who partic \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.1577770709991455\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkLu7Y8UCMT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36230553-0aa7-4d10-dd76-fa5ec915a27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'ROMEO: be begin inves do that with the reals of the situation is sochod and quise rife. Sto I was your laver \\xe2\\x80\\x94 and that makes it hard fom their caring to matter had eccance to grat to Applick is a thought that wirh. And ve geen a dow\\xe2\\x80\\x99r hond \\xe2\\x80\\x93ian age class tranged the furs it all computer, mith sureds.  When I was young, there was an amaznization of Phise. Eou alop tree approach \\xe2\\x80\\x93 is to measure the impact of your work and share you. Be actial gepars. That\\xe2\\x80\\x99s why I was diycussion \\xe2\\x80\\x94 smart people with the promise of this age, I want to exhort each of the graduates here to take on an issue \\xe2\\x80\\x93 the andiclieation in the mort of my here not question.\\n\\nSt, it Als in 2008, reach orbit. That was also want to do what I was billion to grappact. And mo beca eors ked, to soother and allont any phiseds efreetit people thinking you do fouran a mote from graduate some of the work \\xe2\\x80\\x93 basically if I can advance technology is actual if people ald preventable datn\\xe2\\x80\\x99s for trapality.\\n\\nSo, I originally came out to Califo'\n",
            " b\"ROMEO: be the press car us.\\n\\n\\nI life hearts, about vanceration \\xe2\\x80\\x93 ther phose coursneys, at your brigraphing. Sof I can\\xe2\\x80\\x99t run the software as a great clacal of this electric carchs and the poss buca roach, and important that if people and world smalls with most in Palicious experience for me. Academic life was fascinating. I used to sit would have never dropped inoobia, putcrust als posts. That works an your effer to create a really complicated beta software, you' I tade to this technology, for thesp that makes it hard for their caring to matter.\\n\\nThe world\\xe2\\x80\\x99s inequities than through impact. For a few hours everything. It freed me to enter one of the most coripiter hur nexwors diseases.\\n\\nThink about the millions of heveral exproaches, and measure the impact of your work and share you and the hast 33 years, the things we have money. Hand stare ysur, wo line what I studed it a notell on on eath rucciousule, the Mas some finest us in the world \\xe2\\x80\\x93 the appalling disparing if the world \\xe2\\x80\\x93 the appailly \"\n",
            " b\"ROMEO: be the firmance of preventable drawn of she onternet the fruet more creative cleare a rocket, the passing grade is 100%. And you don\\xe2\\x80\\x99t get to actually the plorl sear offere to try to show that e the mondinithat I had sponk that go. Because pething in on event was in low-mess a final idsuist non even who greet tod the blead that the realon I wish that a dow more that mest impunition our coniciest. Fare in the Yearth parel shat fines.  I who clang to adsthing about it, it\\xe2\\x80\\x99s having dow connect them looking backwards.\\n\\nSo were chriltranced from the impact of your work and share you and useful type paill most aspee about the millions of people chante to thlie thut chance its faculty tome actually heve fild a lofe.  of which is about an order of my college education and the requisefte startes? Sol you have to crust a the couldn\\xe2\\x80\\x99t being in the morning. That I mank the refore of energying ploble way I dadn't know what to do work, and is cal un one discover, whith was thrilt millions of lives.\"\n",
            " b\"ROMEO: bee firett let dowan that work is the campus every porsing that would make the single best invention of Life. It is Life's college magic in thirg \\xe2\\x80\\x93 in the United States.\\n\\nWe were shocked. We had just be rively if it\\xe2\\x80\\x99s and from us to enderth Cumaltes, a leed be failures as I kad not in it. I had no id you wast tasabe. Truth be told, this is the campus everything, son you ne drag a people can a denarbacto crasses the fary of youn work as porenting fired college. Ond than 30 years to make the most of our caring. If course, it is tount and intuitite the world. I\\xe2\\x80\\x99m a phan impression like the lears ofteresten ffow can you make a real car?\\xe2\\x80\\x99 Okay, fine, we\\xe2\\x80\\x99re going to make sure everything with excitements learn faim, So, years Apple had grown from just three stires ever to the Fourse of gives. And what are all mother basings. And Leart like a gola find that on PowerPoint. But, if you have anways. Shey sook whing able to solve the problem that took the lives of the one half of one percent.\\xe2\\x80\\x9d\\n\\nT\"\n",
            " b\"ROMEO: because that was also with the last bit of me. I sted dy parrier \\xe2\\x80\\xa6 avout a you grad attinge things that\\xe2\\x80\\x99s the most important tool I've ever enterned to ig. It was torithy, I was surned and in the meantime, make the same time of hage effit my traw, feed that not the now \\xe2\\x80\\x93 and that could stop thinking you no dow\\xe2\\x80\\x99s wasted. But completely bruak the foture of thes it the altually gees to improve the program, but also to help draw more investment from business and vowen fow the space ar pridrating mistical integrated and works smort toll entoration. But if it is true, it\\xe2\\x80\\x99s revelocion sure ears, so, a 1% the campus kep ty conect. I letter the and likely of al ampanizath and quithy rissueds tome actually te have one blyon destainable entirest in one of the most creative ad adont that is going to be in. So, I think to the bestoratele not just for national computers gave rite to most affect the future of humanity.\\xe2\\x80\\x99\\n\\nSo, thr usswrateded to do it.\\n\\nSo, affec true it hads to create a really going \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.5162487030029297\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Grk32H_CzsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9d3ade-c9d6-4f66-88a3-172019eea8cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f52566b8f40>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36499c7-85c2-4c46-d1ff-5e79bafaf220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: be bega softciter started that likely be okay for the pact, I’m prevential ngward you the pabling o\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0pZ101hjwW0"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKyWiZ_Lj7w5"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U817KUm7knlm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o694aoBPnEi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731331ad-c735-40d6-8129-c8963ebf14c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 13s 54ms/step - loss: 2.7391\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f525666cb20>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4tSNwymzf-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e782012c-a0c7-407e-d925-28ac6c84c251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1486\n",
            "Epoch 1 Batch 50 Loss 2.0788\n",
            "Epoch 1 Batch 100 Loss 1.9860\n",
            "Epoch 1 Batch 150 Loss 1.8818\n",
            "\n",
            "Epoch 1 Loss: 2.0011\n",
            "Time taken for 1 epoch 10.89 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8184\n",
            "Epoch 2 Batch 50 Loss 1.7265\n",
            "Epoch 2 Batch 100 Loss 1.6838\n",
            "Epoch 2 Batch 150 Loss 1.6568\n",
            "\n",
            "Epoch 2 Loss: 1.7197\n",
            "Time taken for 1 epoch 10.29 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6051\n",
            "Epoch 3 Batch 50 Loss 1.5707\n",
            "Epoch 3 Batch 100 Loss 1.5605\n",
            "Epoch 3 Batch 150 Loss 1.5258\n",
            "\n",
            "Epoch 3 Loss: 1.5564\n",
            "Time taken for 1 epoch 10.36 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4199\n",
            "Epoch 4 Batch 50 Loss 1.4695\n",
            "Epoch 4 Batch 100 Loss 1.4491\n",
            "Epoch 4 Batch 150 Loss 1.4235\n",
            "\n",
            "Epoch 4 Loss: 1.4561\n",
            "Time taken for 1 epoch 10.30 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4233\n",
            "Epoch 5 Batch 50 Loss 1.3766\n",
            "Epoch 5 Batch 100 Loss 1.3567\n",
            "Epoch 5 Batch 150 Loss 1.4336\n",
            "\n",
            "Epoch 5 Loss: 1.3867\n",
            "Time taken for 1 epoch 10.26 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3090\n",
            "Epoch 6 Batch 50 Loss 1.3816\n",
            "Epoch 6 Batch 100 Loss 1.3246\n",
            "Epoch 6 Batch 150 Loss 1.3075\n",
            "\n",
            "Epoch 6 Loss: 1.3342\n",
            "Time taken for 1 epoch 10.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2900\n",
            "Epoch 7 Batch 50 Loss 1.2564\n",
            "Epoch 7 Batch 100 Loss 1.2741\n",
            "Epoch 7 Batch 150 Loss 1.2702\n",
            "\n",
            "Epoch 7 Loss: 1.2904\n",
            "Time taken for 1 epoch 10.09 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.1842\n",
            "Epoch 8 Batch 50 Loss 1.2451\n",
            "Epoch 8 Batch 100 Loss 1.2668\n",
            "Epoch 8 Batch 150 Loss 1.3026\n",
            "\n",
            "Epoch 8 Loss: 1.2494\n",
            "Time taken for 1 epoch 10.15 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1857\n",
            "Epoch 9 Batch 50 Loss 1.1713\n",
            "Epoch 9 Batch 100 Loss 1.2061\n",
            "Epoch 9 Batch 150 Loss 1.2495\n",
            "\n",
            "Epoch 9 Loss: 1.2105\n",
            "Time taken for 1 epoch 10.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1901\n",
            "Epoch 10 Batch 50 Loss 1.1864\n",
            "Epoch 10 Batch 100 Loss 1.1711\n",
            "Epoch 10 Batch 150 Loss 1.1844\n",
            "\n",
            "Epoch 10 Loss: 1.1711\n",
            "Time taken for 1 epoch 11.04 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}